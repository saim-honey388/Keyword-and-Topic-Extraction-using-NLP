{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Key Word Extraction using NLP"
      ],
      "metadata": {
        "id": "prl67Vk0VMsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Libraries"
      ],
      "metadata": {
        "id": "jOZsO1rLVPwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1kvoWeLU2_I",
        "outputId": "ccb4e544-8862-4727-d488-c656515dfe0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install requests spacy beautifulsoup4 gensim nltk PyPDF2 python-docx\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import PyPDF2\n",
        "import docx\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Extraction from URL,PDF, and  Word Files"
      ],
      "metadata": {
        "id": "1ANtSHz4V-Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to extract Text\n",
        "# Function to extract text from a webpage\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        article_text = \" \".join([p.text for p in soup.find_all('p')])\n",
        "        return article_text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to extract text from a word file\n",
        "def extract_text_from_word(word_file):\n",
        "    doc = docx.Document(word_file)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n"
      ],
      "metadata": {
        "id": "UNsUcklDU6Vc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing and Key Phrase Extraction"
      ],
      "metadata": {
        "id": "UXcO5wenWEXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function to clean and tokenize text\n",
        "\n",
        "#Preprocessing\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
        "\n",
        "# Function to extract key phrases using TF-IDF\n",
        "def extract_key_phrases(text, top_n=10):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(3, 5), stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray().flatten()\n",
        "    sorted_phrases = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
        "    return [phrase for phrase, _ in sorted_phrases[:top_n]]\n"
      ],
      "metadata": {
        "id": "sH-yOf1LWOJy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic Extraction Using LDA"
      ],
      "metadata": {
        "id": "4bWboYbYWW6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract topics using LDA\n",
        "def extract_topics(text, num_topics=3, num_words=5):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    dictionary = corpora.Dictionary([tokens])\n",
        "    corpus = [dictionary.doc2bow(tokens)]\n",
        "\n",
        "    # Training our LDA model\n",
        "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "    return lda_model.print_topics(num_words=num_words)\n"
      ],
      "metadata": {
        "id": "q1S5He-wWau1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key phrases and topics extraction"
      ],
      "metadata": {
        "id": "XW_ynxU1WgHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract key phrases and topics\n",
        "def extract_from_source(source_type, source, top_n=10, num_topics=3, num_words=5):\n",
        "    text = None\n",
        "    if source_type == \"url\":\n",
        "        text = extract_text_from_url(source)\n",
        "    elif source_type == \"pdf\":\n",
        "        text = extract_text_from_pdf(source)\n",
        "    elif source_type == \"word\":\n",
        "        text = extract_text_from_word(source)\n",
        "    else:\n",
        "        print(\"Invalid source type!\")\n",
        "        return\n",
        "\n",
        "    if text:\n",
        "        cleaned_text = preprocess_text(text)\n",
        "        # Extract and display key phrases\n",
        "        print(\"\\nExtracting key phrases:\")\n",
        "        key_phrases = extract_key_phrases(cleaned_text, top_n)\n",
        "        print(f\"Key Phrases: {key_phrases}\")\n",
        "        # Extract and display topics\n",
        "        print(\"\\nExtracting topics:\")\n",
        "        topics = extract_topics(cleaned_text, num_topics, num_words)\n",
        "        for idx, topic in enumerate(topics):\n",
        "            print(f\"Topic {idx + 1}: {topic}\")\n",
        "    else:\n",
        "        print(\"Failed to extract text from the source.\")\n"
      ],
      "metadata": {
        "id": "H0KWQKBRWwfu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function and User Interface"
      ],
      "metadata": {
        "id": "qYRhcP4-Wx3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to handle the user interface\n",
        "def main():\n",
        "    while True:\n",
        "        print(\"\\nSelect the input method:\")\n",
        "        print(\"1. URL\")\n",
        "        print(\"2. PDF file\")\n",
        "        print(\"3. Word file\")\n",
        "        print(\"4. Exit\")\n",
        "        choice = input(\"Enter your choice: \")\n",
        "        if choice == \"1\":\n",
        "            url = input(\"Enter the URL: \")\n",
        "            extract_from_source(\"url\", url)\n",
        "        elif choice == \"2\":\n",
        "            pdf_file_path = input(\"Enter the path to the PDF file: \")\n",
        "            try:\n",
        "                with open(pdf_file_path, \"rb\") as pdf_file:\n",
        "                    extract_from_source(\"pdf\", pdf_file)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"File not found: {pdf_file_path}\")\n",
        "        elif choice == \"3\":\n",
        "            word_file_path = input(\"Enter the path to the Word file: \")\n",
        "            try:\n",
        "                with open(word_file_path, \"rb\") as word_file:\n",
        "                    extract_from_source(\"word\", word_file)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"File not found: {word_file_path}\")\n",
        "        elif choice == \"4\":\n",
        "            print(\"Program Exit.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please select a valid option\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tURS8C5UW_Op",
        "outputId": "dc351dfc-9fcb-4c21-aad8-462ef1660e20"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Select the input method:\n",
            "1. URL\n",
            "2. PDF file\n",
            "3. Word file\n",
            "4. Exit\n",
            "Enter your choice: 1\n",
            "Enter the URL: https://www.nbcnews.com/\n",
            "\n",
            "Extracting key phrases:\n",
            "Key Phrases: ['access feature free', 'access feature free account', 'access feature free account valid', 'account valid zip', 'account valid zip code', 'account valid zip code nbc', 'advertisement access feature', 'advertisement access feature free', 'advertisement access feature free account', 'alert time advertisement']\n",
            "\n",
            "Extracting topics:\n",
            "Topic 1: (0, '0.056*\"Advertisement\" + 0.056*\"account\" + 0.056*\"ZIP\" + 0.056*\"alert\" + 0.056*\"News\"')\n",
            "Topic 2: (1, '0.124*\"NBC\" + 0.086*\"feature\" + 0.049*\"Sections\" + 0.049*\"UNIVERSAL\" + 0.049*\"Access\"')\n",
            "Topic 3: (2, '0.056*\"NBC\" + 0.056*\"feature\" + 0.056*\"News\" + 0.056*\"new\" + 0.056*\"Profile\"')\n",
            "\n",
            "Select the input method:\n",
            "1. URL\n",
            "2. PDF file\n",
            "3. Word file\n",
            "4. Exit\n",
            "Enter your choice: 2\n",
            "Enter the path to the PDF file: /content/Network Security (ch-8).pdf\n",
            "\n",
            "Extracting key phrases:\n",
            "Key Phrases: ['bob public key', 'firewall network security', 'integrity point authentication', 'layer security ipsec', 'network security roadmap', 'network security roadmap network', 'network security roadmap network security', 'roadmap network security', 'security firewall network', 'security firewall network security']\n",
            "\n",
            "Extracting topics:\n",
            "Topic 1: (0, '0.003*\"Security\" + 0.003*\"Alice\" + 0.003*\"key\" + 0.002*\"Network\" + 0.002*\"security\"')\n",
            "Topic 2: (1, '0.034*\"key\" + 0.032*\"Security\" + 0.032*\"Network\" + 0.028*\"Alice\" + 0.016*\"Bob\"')\n",
            "Topic 3: (2, '0.003*\"key\" + 0.002*\"Alice\" + 0.002*\"Security\" + 0.002*\"Network\" + 0.002*\"public\"')\n",
            "\n",
            "Select the input method:\n",
            "1. URL\n",
            "2. PDF file\n",
            "3. Word file\n",
            "4. Exit\n",
            "Enter your choice: 3\n",
            "Enter the path to the Word file: /content/OOP PROJECT REPORT.docx\n",
            "\n",
            "Extracting key phrases:\n",
            "Key Phrases: ['able atleast usable', 'able atleast usable social', 'able atleast usable social medium', 'abstract feature winku', 'abstract feature winku app', 'abstract feature winku app scope', 'abstract real winku', 'abstract real winku application', 'abstract real winku application online', 'abstraction conclusion fully']\n",
            "\n",
            "Extracting topics:\n",
            "Topic 1: (0, '0.006*\"application\" + 0.006*\"user\" + 0.005*\"provide\" + 0.005*\"social\" + 0.005*\"post\"')\n",
            "Topic 2: (1, '0.035*\"application\" + 0.023*\"user\" + 0.015*\"like\" + 0.015*\"friend\" + 0.015*\"member\"')\n",
            "Topic 3: (2, '0.006*\"application\" + 0.005*\"user\" + 0.005*\"post\" + 0.005*\"member\" + 0.005*\"friend\"')\n",
            "\n",
            "Select the input method:\n",
            "1. URL\n",
            "2. PDF file\n",
            "3. Word file\n",
            "4. Exit\n",
            "Enter your choice: 4\n",
            "Program Exit.\n"
          ]
        }
      ]
    }
  ]
}